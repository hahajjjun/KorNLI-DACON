{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet-cu101\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece==0.1.85\n",
        "!pip install transformers==2.1.1\n",
        "!pip install torch==1.3.1\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diAMBcdUGSgZ",
        "outputId": "8ba53669-9a20-4342-dc13-296667abe48e"
      },
      "id": "diAMBcdUGSgZ",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mxnet-cu101\n",
            "  Downloading mxnet_cu101-1.9.0-py3-none-manylinux2014_x86_64.whl (358.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 358.1 MB 5.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu101) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n",
            "Installing collected packages: graphviz, mxnet-cu101\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-cu101-1.9.0\n",
            "Collecting gluonnlp\n",
            "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
            "\u001b[K     |████████████████████████████████| 344 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.19.5)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.27)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (3.0.7)\n",
            "Building wheels for collected packages: gluonnlp\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595732 sha256=721162d23562c7a3227751f47cf8d6188104311a513d0ca4cf3be2988d4d3524\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n",
            "Successfully built gluonnlp\n",
            "Installing collected packages: gluonnlp\n",
            "Successfully installed gluonnlp-0.10.0\n",
            "Collecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 5.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n",
            "Collecting transformers==2.1.1\n",
            "  Downloading transformers-2.1.1-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (2.23.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.20.54-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from transformers==2.1.1) (4.62.3)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.1-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting botocore<1.24.0,>=1.23.54\n",
            "  Downloading botocore-1.23.54-py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 39.0 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.54->boto3->transformers==2.1.1) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 47.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.54->boto3->transformers==2.1.1) (1.15.0)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.1.1) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.1.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.1.1) (1.1.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sacremoses, boto3, transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.20.54 botocore-1.23.54 jmespath-0.10.0 s3transfer-0.5.1 sacremoses-0.0.47 transformers-2.1.1 urllib3-1.25.11\n",
            "Collecting torch==1.3.1\n",
            "  Downloading torch-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (734.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 734.6 MB 18 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.3.1) (1.19.5)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.3.1 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.3.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.3.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.3.1\n",
            "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
            "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-9ekuc3v1\n",
            "  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-9ekuc3v1\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (1.20.54)\n",
            "Requirement already satisfied: gluonnlp>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.10.0)\n",
            "Collecting mxnet>=1.4.0\n",
            "  Downloading mxnet-1.9.0-py3-none-manylinux2014_x86_64.whl (47.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.3 MB 124 kB/s \n",
            "\u001b[?25hCollecting onnxruntime==1.8.0\n",
            "  Downloading onnxruntime-1.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 34.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from kobert==0.2.3) (0.1.85)\n",
            "Collecting torch>=1.7.0\n",
            "  Downloading torch-1.10.2-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.6 MB/s eta 0:00:30tcmalloc: large alloc 1147494400 bytes == 0x55fc684f8000 @  0x7f3becde2615 0x55fc2fb4a3bc 0x55fc2fc2b18a 0x55fc2fb4d1cd 0x55fc2fc3fb3d 0x55fc2fbc1458 0x55fc2fbbc02f 0x55fc2fb4eaba 0x55fc2fbc12c0 0x55fc2fbbc02f 0x55fc2fb4eaba 0x55fc2fbbdcd4 0x55fc2fc40986 0x55fc2fbbd350 0x55fc2fc40986 0x55fc2fbbd350 0x55fc2fc40986 0x55fc2fbbd350 0x55fc2fb4ef19 0x55fc2fb92a79 0x55fc2fb4db32 0x55fc2fbc11dd 0x55fc2fbbc02f 0x55fc2fb4eaba 0x55fc2fbbdcd4 0x55fc2fbbc02f 0x55fc2fb4eaba 0x55fc2fbbceae 0x55fc2fb4e9da 0x55fc2fbbd108 0x55fc2fbbc02f\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 1.9 kB/s \n",
            "\u001b[?25hCollecting transformers>=4.8.1\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 44.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (2.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (1.19.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime==1.8.0->kobert==0.2.3) (3.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (21.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (0.29.27)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.2.3) (2.23.0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->kobert==0.2.3) (0.8.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->kobert==0.2.3) (3.10.0.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (0.0.47)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (3.4.2)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 37.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (4.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.8.1->kobert==0.2.3) (4.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp>=0.6.0->kobert==0.2.3) (3.0.7)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->kobert==0.2.3) (0.5.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->kobert==0.2.3) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.24.0,>=1.23.54 in /usr/local/lib/python3.7/dist-packages (from boto3->kobert==0.2.3) (1.23.54)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.54->boto3->kobert==0.2.3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.54->boto3->kobert==0.2.3) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.8.1->kobert==0.2.3) (3.7.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (1.1.0)\n",
            "Building wheels for collected packages: kobert\n",
            "  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15449 sha256=e993c04dfe952cc7cc5348c62da1a274050fb8c9b101495efa5d3670439f9987\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_8te71t9/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\n",
            "Successfully built kobert\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, torch, onnxruntime, mxnet, kobert\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 2.1.1\n",
            "    Uninstalling transformers-2.1.1:\n",
            "      Successfully uninstalled transformers-2.1.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.3.1\n",
            "    Uninstalling torch-1.3.1:\n",
            "      Successfully uninstalled torch-1.3.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.10.2 which is incompatible.\u001b[0m\n",
            "Successfully installed huggingface-hub-0.4.0 kobert-0.2.3 mxnet-1.9.0 onnxruntime-1.8.0 pyyaml-6.0 tokenizers-0.11.4 torch-1.10.2 transformers-4.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "75e43b6d",
      "metadata": {
        "id": "75e43b6d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GhRDKAIJprw",
        "outputId": "ff540de9-50eb-4e51-cafb-6e0993f5c42b"
      },
      "id": "8GhRDKAIJprw",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6b1e8cc",
      "metadata": {
        "id": "a6b1e8cc"
      },
      "source": [
        "# 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e5a811ed",
      "metadata": {
        "id": "e5a811ed"
      },
      "outputs": [],
      "source": [
        "TRAIN = '/content/drive/MyDrive/DACON_MONTHLYNLI/train_data.csv'\n",
        "TEST = '/content/drive/MyDrive/DACON_MONTHLYNLI/test_data.csv'\n",
        "SS = '/content/drive/MyDrive/DACON_MONTHLYNLI/sample_submission.csv'\n",
        "\n",
        "train = pd.read_csv(TRAIN)\n",
        "test = pd.read_csv(TEST)\n",
        "submission = pd.read_csv(SS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "45ed087c",
      "metadata": {
        "id": "45ed087c"
      },
      "outputs": [],
      "source": [
        "max_len = 70\n",
        "batch_size = 64    # Batch size 감소\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate = 5e-5\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9bc6fa0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bc6fa0c",
        "outputId": "97179e84-bb22-4d0e-b87f-0e480df6ddcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/.cache/kobert_v1.zip[██████████████████████████████████████████████████]\n",
            "/content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n",
            "using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ],
      "source": [
        "bertmodel, vocab = get_pytorch_kobert_model(cachedir = \".cache\")\n",
        "\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a4157a9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4157a9f",
        "outputId": "2ad4aa36-bb8e-4ed5-df65-89f996db2a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['contradiction' 'entailment' 'neutral']\n"
          ]
        }
      ],
      "source": [
        "print(pd.unique(train[\"label\"]))\n",
        "\n",
        "label_dict = {\"entailment\" : 0, \"contradiction\" : 1, \"neutral\" : 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7e355740",
      "metadata": {
        "id": "7e355740"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sen_idx, label_idx, bert_tokenizer, max_len,\n",
        "                pad, pair, mode = \"train\"):\n",
        "        self.mode = mode\n",
        "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length = max_len, pad = pad, pair = pair)\n",
        "        if self.mode == \"train\":\n",
        "            self.sentence = [transform(i[sen_idx]) for i in dataset]   # pair=True를 위해 수정. [] 제거\n",
        "            self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "            \n",
        "        else:\n",
        "            self.sentence =  [transform(i[sen_idx]) for i in dataset]   # pair=True를 위해 수정. [] 제거\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        if self.mode == 'train':\n",
        "            return (self.sentence[i] +  (self.labels[i], ))\n",
        "        else:\n",
        "            return self.sentence[i]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return (len(self.sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ff06aa7",
      "metadata": {
        "id": "7ff06aa7"
      },
      "source": [
        "# Tokenizing\n",
        "* Baseline 코드를 돌려 보던 중 Tokeninzing에서 이상한 점을 발견\n",
        "* [CLS]와 [SEP]가 토큰으로 인식 되지 않고 분해되는 것을 발견함\n",
        "* Fair를 활용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e2335128",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2335128",
        "outputId": "1fafb7b4-2a57-4270-c51c-07508114786a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁[',\n",
              " '▁C',\n",
              " 'LS',\n",
              " '▁',\n",
              " ']',\n",
              " '▁씨',\n",
              " '름',\n",
              " '은',\n",
              " '▁상',\n",
              " '고',\n",
              " '시대',\n",
              " '로부터',\n",
              " '▁전해',\n",
              " '져',\n",
              " '▁내려',\n",
              " '오는',\n",
              " '▁남자',\n",
              " '들의',\n",
              " '▁대표적인',\n",
              " '▁놀',\n",
              " '이',\n",
              " '로서',\n",
              " '▁',\n",
              " ',',\n",
              " '▁소',\n",
              " '년',\n",
              " '이나',\n",
              " '▁장',\n",
              " '정',\n",
              " '들이',\n",
              " '▁넓',\n",
              " '고',\n",
              " '▁평',\n",
              " '평',\n",
              " '한',\n",
              " '▁백',\n",
              " '사',\n",
              " '장',\n",
              " '이나',\n",
              " '▁마',\n",
              " '당',\n",
              " '에서',\n",
              " '▁모여',\n",
              " '▁서로',\n",
              " '▁힘',\n",
              " '과',\n",
              " '▁슬',\n",
              " '기를',\n",
              " '▁',\n",
              " '겨',\n",
              " '루',\n",
              " '는',\n",
              " '▁것이다',\n",
              " '▁',\n",
              " '.',\n",
              " '▁[',\n",
              " '▁S',\n",
              " 'E',\n",
              " 'P',\n",
              " '▁',\n",
              " ']',\n",
              " '▁씨',\n",
              " '름',\n",
              " '의',\n",
              " '▁여자',\n",
              " '들의',\n",
              " '▁놀',\n",
              " '이',\n",
              " '이다',\n",
              " '▁',\n",
              " '.',\n",
              " '▁[',\n",
              " '▁S',\n",
              " 'E',\n",
              " 'P',\n",
              " '▁',\n",
              " ']']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# [CLS], [SEP] 토큰 분해\n",
        "text1 = train['premise'][0]\n",
        "text2 = train['hypothesis'][0]\n",
        "concat_text = \"[CLS]\" + text1 + \"[SEP] \" + text2 + \"[SEP]\"\n",
        "tok(concat_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1c922692",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c922692",
        "outputId": "69e16e39-3ba4-4ca2-c349-ee2cbaa8d11a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  2, 702,   3,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
              "          1,   1,   1,   1,   1], dtype=int32),\n",
              " array(3, dtype=int32),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "transform = nlp.data.BERTSentenceTransform(tok, max_seq_length = max_len, pad = True, pair = False)\n",
        "\n",
        "# [CLS]와 [SEP]는 자동으로 붙여주는거 같음\n",
        "transform(concat_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f69576cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f69576cc",
        "outputId": "a53c1f74-4939-429f-c676-4ed2a2eb0bdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([   2, 3088, 6117, 7086, 2658, 5439, 6708, 6080, 4059, 7245, 1442,\n",
              "        6965, 1423, 5939, 1678, 1504, 7096, 6081,  517,   46, 2822, 5712,\n",
              "        7098, 3954, 7227, 5940, 1459, 5439, 4841, 7724, 7828, 2298, 6493,\n",
              "        7178, 7098, 1907, 5804, 6903, 2064, 2720, 5211, 5468, 2948, 5573,\n",
              "         517, 5411, 6095, 5760,  913,  517,   54,    3, 3088, 6117, 7095,\n",
              "        3318, 5939, 1504, 7096, 7100,  517,   54,    3,    1,    1,    1,\n",
              "           1,    1,    1,    1], dtype=int32),\n",
              " array(63, dtype=int32),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "        0, 0, 0, 0], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Fair를 활용한 tokenizing\n",
        "text1 = train['premise'][0]\n",
        "text2 = train['hypothesis'][0]\n",
        "\n",
        "# pair True 설정\n",
        "transform = nlp.data.BERTSentenceTransform(tok, max_seq_length = max_len, pad = True, pair = True)\n",
        "transform([text1, text2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f226e813",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f226e813",
        "outputId": "2db48920-431b-48d7-9095-7ab7d6279b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24998/24998 [00:00<00:00, 27081.02it/s]\n",
            "100%|██████████| 1666/1666 [00:00<00:00, 36108.65it/s]\n"
          ]
        }
      ],
      "source": [
        "# Fair를 활용\n",
        "# train[\"premise_\"] = \"[CLS]\" + train[\"premise\"] + \"[SEP]\"\n",
        "# train[\"hypothesis_\"] = train[\"hypothesis\"] + \"[SEP]\"\n",
        "\n",
        "# test[\"premise_\"] = \"[CLS]\" + test[\"premise\"] + \"[SEP]\"\n",
        "# test[\"hypothesis_\"] = test[\"hypothesis\"] + \"[SEP]\"\n",
        "\n",
        "# train[\"text_sum\"] = train.premise_ + \" \" + train.hypothesis_\n",
        "# test[\"text_sum\"] = test.premise_ + \" \" + test.hypothesis_\n",
        "\n",
        "train_content = []\n",
        "test_content = []\n",
        "\n",
        "for idx in tqdm(train.index):\n",
        "    train_content.append(list([[train.loc[idx, 'premise'], train.loc[idx, 'hypothesis']], label_dict[train.loc[idx, 'label']]]))\n",
        "    \n",
        "for idx in tqdm(test.index):\n",
        "    test_content.append(list([[test.loc[idx, 'premise'], test.loc[idx, 'hypothesis']]]))\n",
        "    \n",
        "dataset_train = train_content[:20000]\n",
        "dataset_valid = train_content[20000:]\n",
        "dataset_test = test_content\n",
        "\n",
        "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, True, mode = \"train\")\n",
        "data_valid = BERTDataset(dataset_valid, 0, 1, tok, max_len, True, True, mode = \"train\")\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, True, mode = \"test\")\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size = batch_size, num_workers = 0)\n",
        "valid_dataloader = torch.utils.data.DataLoader(data_valid, batch_size = batch_size, num_workers = 0)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size = batch_size, num_workers = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b165f21",
      "metadata": {
        "id": "1b165f21"
      },
      "source": [
        "# 모델 만들기\n",
        "* Baseline 코드와 동일\n",
        "* Kobert 공식 튜토리얼 문서에 따라 warmup scheduler 적용\n",
        "* Batch size를 128로 동일하게 하고 싶었으나 메모리 문제로 64로 줄임(GPU 지원 받습니다.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "32c7f06a",
      "metadata": {
        "id": "32c7f06a"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert, hidden_size = 768, num_classes=3, dr_rate=None, params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "    \n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)\n",
        "\n",
        "        \n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "07027ea5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07027ea5",
        "outputId": "962a36ae-b47a-4a4a-c72a-55739af85c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "model = BERTClassifier(bertmodel, dr_rate = 0.5).to(device)\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# warup scheduler\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "ccc77a10",
      "metadata": {
        "id": "ccc77a10"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1c16348b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c16348b",
        "outputId": "181b6388-b22e-4336-ad6b-058f307774a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [07:03<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 train acc 0.5839656549520766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:39<00:00,  2.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 valid acc 0.7573180379746836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [07:03<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 train acc 0.8048123003194888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:39<00:00,  2.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2 valid acc 0.7844804852320676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [07:03<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 train acc 0.8871805111821086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:39<00:00,  2.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3 valid acc 0.7807225738396625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [07:04<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4 train acc 0.9371505591054313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:39<00:00,  2.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4 valid acc 0.8072257383966245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [07:03<00:00,  1.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5 train acc 0.9628594249201278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:39<00:00,  2.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5 valid acc 0.8094013713080169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    valid_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()    # scheduler 적용\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    model.eval()\n",
        "    \n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in tqdm(enumerate(valid_dataloader), total=len(valid_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        valid_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} valid acc {}\".format(e+1, valid_acc / (batch_id+1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adbf1a17",
      "metadata": {
        "id": "adbf1a17"
      },
      "source": [
        "# 추론 및 제출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "31b12a8a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31b12a8a",
        "outputId": "3203d9e3-dab6-4b24-efd1-5fb71447fc59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27/27 [00:12<00:00,  2.10it/s]\n"
          ]
        }
      ],
      "source": [
        "result = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch_id, (token_ids, valid_length, segment_ids) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        result.append(model(token_ids, valid_length, segment_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "cafe32ca",
      "metadata": {
        "id": "cafe32ca"
      },
      "outputs": [],
      "source": [
        "result_ = []\n",
        "for i in result:\n",
        "    for j in i:\n",
        "        result_.append(int(torch.argmax(j)))\n",
        "        \n",
        "out = [list(label_dict.keys())[_] for _ in result_]\n",
        "\n",
        "submission[\"label\"] = out\n",
        "\n",
        "submission.to_csv(\"kobert_baseline.csv\", index = False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "kobert_베이스라인 수정 public score 0.736.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}